{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "torch-starter.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN+m3yScm+aEoE3kSsVuKdz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wandb/davis-contest/blob/main/colabs/starter_torch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9MDnS5kwVze"
      },
      "source": [
        "# Instructions and Starter Code for Submitting Results in the DAVIS Contest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHSX1I__VJSW"
      },
      "source": [
        "%%capture\n",
        "\n",
        "!pip install wandb\n",
        "!pip install --ignore-installed git+https://github.com/wandb/davis-contest.git#egg=contest\n",
        "!pip install ptflops pytorch_lightning"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IxaxWQGxU9pi"
      },
      "source": [
        "import os \r\n",
        "\r\n",
        "import wandb\r\n",
        "\r\n",
        "import contest\r\n",
        "from contest.utils import clips, paths"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vtUNLAmqze-T"
      },
      "source": [
        "## 0️⃣ Create a Weights & Biases account if you don't have one."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHogIiFVhXjv"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aoTHl37XxAS9"
      },
      "source": [
        "## 1️⃣ Download the training data from Weights & Biases"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXF0H0kpVRAq"
      },
      "source": [
        "entity = \"charlesfrye\"\r\n",
        "project = \"davis\"\r\n",
        "mode = \"train\"\r\n",
        "tag = \"latest\"\r\n",
        "\r\n",
        "training_data_artifact_name = os.path.join(entity, project, f\"davis2016-{mode}\") + \":\" + tag\r\n",
        "training_data_artifact_name"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pyTXkGNEU8wo"
      },
      "source": [
        "with wandb.init(project=project, job_type=\"download\") as run:\r\n",
        "  training_data_artifact = run.use_artifact(training_data_artifact_name)\r\n",
        "  training_data_dir = training_data_artifact.download()\r\n",
        "  print(\"\\ntraining data downloaded to \" + training_data_dir)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aw9pYa8vxEGJ"
      },
      "source": [
        "### Viewing the Dataset in Weights & Biases"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-NUi6-DWYZ-"
      },
      "source": [
        "Link to dsviz version, include screenshots."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMuifb6FxJlB"
      },
      "source": [
        "## 2️⃣ Define and train a model on the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3ZHadq5LAQ1"
      },
      "source": [
        "### Splitting up the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-TLlyygKxZg"
      },
      "source": [
        "print(clips.split_on_clips.__doc__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JI-TJxN5A_U7"
      },
      "source": [
        "print(contest.torch.data.VidSegDataModule.__doc__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ePM-lvJBMNP"
      },
      "source": [
        "print(contest.torch.data.VidSegDataset.__doc__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4a4iHs1Kss6"
      },
      "source": [
        "First, set up the validation split, at a clipwise level."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kkak4W9n_W1N"
      },
      "source": [
        "def log_holdout_split(data_artifact, train_split_df, holdout_split_df):\r\n",
        "  log_datasplit_artifact(data_artifact, train_split_df, \"train\")\r\n",
        "  log_datasplit_artifact(data_artifact, holdout_split_df, \"holdout\")\r\n",
        "\r\n",
        "\r\n",
        "def log_datasplit_artifact(data_artifact, split_df, splitname, folder=\"wandb\"):\r\n",
        "  dataset_artifact = wandb.Artifact(name=f\"davis2016-split-{splitname}\", type=\"split-data\")\r\n",
        "  path = os.path.join(folder, splitname + \".json\")\r\n",
        "  split_df.to_json(path)\r\n",
        "  dataset_artifact.add_file(path, \"paths.json\")\r\n",
        "\r\n",
        "  wandb.run.log_artifact(dataset_artifact)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQqBg6VQENhV"
      },
      "source": [
        "config = {\"training_fraction\": 0.8}\r\n",
        "\r\n",
        "with wandb.init(project=project,\r\n",
        "                job_type=\"split-data\", config=config) as run:\r\n",
        "  training_data_artifact = run.use_artifact(training_data_artifact_name)\r\n",
        "  paths_df = paths.artifact_paths(training_data_artifact)\r\n",
        "\r\n",
        "  training_paths_df, holdout_paths_df = clips.split_on_clips(paths_df)\r\n",
        "  log_holdout_split(training_data_artifact,\r\n",
        "                    training_paths_df,\r\n",
        "                    holdout_paths_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ab2u0qDYvUXE"
      },
      "source": [
        "### Model Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tI0BQGk9pEkc"
      },
      "source": [
        "import pytorch_lightning as pl\r\n",
        "import torch\r\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEg0WkTTW3P0"
      },
      "source": [
        "class DummyModel(pl.LightningModule):\r\n",
        "\r\n",
        "  def __init__(self):\r\n",
        "    super().__init__()\r\n",
        "    self.conv = torch.nn.Conv2d(in_channels=3, out_channels=1, kernel_size=1)\r\n",
        "\r\n",
        "  def forward(self, xs):\r\n",
        "    return torch.sigmoid(self.conv(xs))\r\n",
        "\r\n",
        "  def training_step(self, batch, batch_idx):\r\n",
        "    loss = self.forward_on_batch(batch)\r\n",
        "    return loss\r\n",
        "\r\n",
        "  def validation_step(self, batch, batch_idx):\r\n",
        "    loss = self.forward_on_batch(batch)\r\n",
        "    return loss\r\n",
        "\r\n",
        "  def forward_on_batch(self, batch):\r\n",
        "    xs, ys = batch\r\n",
        "    y_hats = self.forward(xs)\r\n",
        "    loss = F.binary_cross_entropy(y_hats, ys)\r\n",
        "    return loss\r\n",
        "\r\n",
        "  def configure_optimizers(self):\r\n",
        "    return torch.optim.SGD(self.parameters(), lr=0.0)\r\n",
        "\r\n",
        "  def count_params(self):\r\n",
        "    return sum(p.numel() for p in self.parameters())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UPt3qidL7sIN"
      },
      "source": [
        "For a more realistic model, see _this notebook_."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fF4sq2BrKaZk"
      },
      "source": [
        "### Training Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VfCj56rvLEh1"
      },
      "source": [
        "#### Training the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hia3dlPkefU1"
      },
      "source": [
        "model_artifact_name = \"dummy-baseline\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFKaT-Fr0Ad6"
      },
      "source": [
        "config = {\"batch_size\": 32,\r\n",
        "          \"max_epochs\": 1,\r\n",
        "          \"gpus\": 1}\r\n",
        "\r\n",
        "with wandb.init(project=project, config=config, job_type=\"train\") as run:\r\n",
        "\r\n",
        "  training_data_artifact = run.use_artifact(training_data_artifact_name)\r\n",
        "  training_data_artifact.download()\r\n",
        "\r\n",
        "  trainsplit_artifact = run.use_artifact(\"davis2016-split-train:latest\")\r\n",
        "  trainsplit_paths = paths.get_paths(trainsplit_artifact)\r\n",
        "\r\n",
        "  holdoutsplit_artifact = run.use_artifact(\"davis2016-split-holdout:latest\")\r\n",
        "  holdoutsplit_paths = paths.get_paths(holdoutsplit_artifact)\r\n",
        "\r\n",
        "  datamodule = contest.torch.data.VidSegDataModule(\r\n",
        "      trainsplit_paths, holdoutsplit_paths,\r\n",
        "      batch_size=wandb.config[\"batch_size\"])\r\n",
        "  datamodule.setup()\r\n",
        "\r\n",
        "  model = DummyModel()\r\n",
        "  wandb.config[\"nparams\"] = contest.torch.profile.count_params(model)\r\n",
        "  wandb.config[\"nflops\"] = contest.torch.profile.count_flops(model, torch.cuda.device(0))\r\n",
        "  \r\n",
        "  logger = pl.loggers.wandb.WandbLogger(experiment=run)\r\n",
        "  logger.watch(model, log_freq=2)\r\n",
        "\r\n",
        "  trainer = pl.Trainer(\r\n",
        "    gpus=wandb.config[\"gpus\"], max_epochs=wandb.config[\"max_epochs\"],\r\n",
        "    logger=logger, log_every_n_steps=1) \r\n",
        "  \r\n",
        "  trainer.fit(model, datamodule)\r\n",
        "\r\n",
        "  model_artifact_id = contest.torch.utils.save_model_to_artifact(\r\n",
        "    model, \"wandb/final_model\", model_artifact_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7gj0eF8yc2S"
      },
      "source": [
        "## 3️⃣ Run your model on the evaluation data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oleltn0vzS6o"
      },
      "source": [
        "Once you've run your model on the evaluation data,\r\n",
        "there's two steps to submission:\r\n",
        "\r\n",
        "1. Log an \"evaluation run\" to W&B, using _this notebook_.\r\n",
        "2. Submit the results to _the benchmark_."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMCkpXv2xCr1"
      },
      "source": [
        "Describe format of the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5xg9fMtO1OJ"
      },
      "source": [
        "evaluation_artifact_name = os.path.join(entity, project, \"davis2016-val\" +\":\" + tag)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s-ISco63Px4H"
      },
      "source": [
        "model_tag = \"latest\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OfZP5_TWdyh1"
      },
      "source": [
        "output_dir = os.path.join(\"outputs\")\r\n",
        "!rm -rf output_dir\r\n",
        "!mkdir -p {output_dir}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RcPArWlSeR9t"
      },
      "source": [
        "result_artifact_name = model_artifact_name + \"-result\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZkNEH5zOOIJ-"
      },
      "source": [
        "with wandb.init(project=project, job_type=\"run-val\") as run:\r\n",
        "  evaluation_data_artifact = run.use_artifact(evaluation_artifact_name)\r\n",
        "  evaluation_data_paths = paths.artifact_paths(evaluation_data_artifact)\r\n",
        "\r\n",
        "  evaluation_dataset = contest.torch.data.VidSegDataset(\r\n",
        "    evaluation_data_paths, has_annotations=False)\r\n",
        "  num_images = len(evaluation_dataset)\r\n",
        "\r\n",
        "  evaluation_dataloader = torch.utils.data.DataLoader(\r\n",
        "    evaluation_dataset, batch_size=1)\r\n",
        "\r\n",
        "  model = contest.torch.utils.load_model_from_artifact(\r\n",
        "    model_artifact_name + \":\" + model_tag, DummyModel) \r\n",
        "\r\n",
        "  print(\"\\n\")\r\n",
        "  device = torch.cuda.device(0)\r\n",
        "  nparams = contest.torch.profile.count_params(model)\r\n",
        "  nflops = contest.torch.profile.count_flops(model, device)\r\n",
        "\r\n",
        "  wandb.log({\"nparams\": nparams, \"nflops\": nflops})\r\n",
        "\r\n",
        "  output_paths = contest.torch.evaluate.run(\r\n",
        "    model, evaluation_dataloader, num_images, output_dir)\r\n",
        "\r\n",
        "  result_artifact = contest.evaluate.make_result_artifact(\r\n",
        "    output_paths, result_artifact_name)\r\n",
        "  run.log_artifact(result_artifact)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xQDEuHMzNjp"
      },
      "source": [
        "## 4️⃣ Submit your results to the leaderboard on Weights & Biases"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGn2Y9wUU2W9"
      },
      "source": [
        "Once you've run an evaluation job like the one above and produced a results Artifact,\r\n",
        "you're almost ready to submit to the contest.\r\n",
        "\r\n",
        "Head over to _this notebook_ for the last two steps."
      ]
    }
  ]
}