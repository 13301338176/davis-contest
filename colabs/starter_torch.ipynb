{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "starter-torch.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9-O1_SJppLC"
      },
      "source": [
        "![header](https://i.imgur.com/sAPM7Yy.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9MDnS5kwVze"
      },
      "source": [
        "# Instructions and Starter Code for the DAVIS Contest - PyTorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdMRDHIyRMNK"
      },
      "source": [
        "This notebook demonstrates how to structure your code\r\n",
        "and results for the DAVIS contest\r\n",
        "by means of an end-to-end example using\r\n",
        "the\r\n",
        "[PyTorch](https://pytorch.org/docs/stable/index.html)\r\n",
        "and\r\n",
        "[PyTorch Lightning](https://pytorch-lightning.readthedocs.io/en/latest/)\r\n",
        "deep learning frameworks.\r\n",
        "See [this colab notebook](http://wandb.me/davis-starter-keras)\r\n",
        "for the same in Tensorflow/Keras.\r\n",
        "\r\n",
        "You should feel free to make use of the code here and in\r\n",
        "[the contest repo](https://github.com/wandb/davis-contest)\r\n",
        "(installed via `pip` below and imported as `contest`)\r\n",
        "to build your data engineering and model training pipelines,\r\n",
        "but that's not strictly necessary to compete in the contest.\r\n",
        "All that you need to do is produce your results\r\n",
        "in an appropriately-formatted\r\n",
        "Weights & Biases [Artifact](https://docs.wandb.ai/artifacts),\r\n",
        "as described below,\r\n",
        "and follow the instructions in the\r\n",
        "[submission notebook](http://wandb.me/davis-submit)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHSX1I__VJSW"
      },
      "source": [
        "%%capture\n",
        "\n",
        "!pip install git+https://github.com/wandb/davis-contest.git#egg=contest[torch]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IxaxWQGxU9pi"
      },
      "source": [
        "import os \r\n",
        "\r\n",
        "import wandb\r\n",
        "\r\n",
        "import contest  # utilities for working with contest data\r\n",
        "from contest.utils import clips, paths"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vtUNLAmqze-T"
      },
      "source": [
        "## 0️⃣ Create a Weights & Biases account if you don't have one."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnLxzI7E20Vp"
      },
      "source": [
        "[Weights & Biases](https://wandb.ai/site)\r\n",
        "is a developer toolkit for machine learning --\r\n",
        "kind of like GitHub, but specialized\r\n",
        "to the particular problems that come up in machine learning.\r\n",
        "\r\n",
        "We'll be using it throughout the contest\r\n",
        "to organize datasets,\r\n",
        "track models during training,\r\n",
        "and evaluate model performance for submission.\r\n",
        "\r\n",
        "Run the cell below to either log in to Weights & Biases\r\n",
        "or create a new account.\r\n",
        "If you're participating in the contest,\r\n",
        "make sure to sign up under your company email address."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hHogIiFVhXjv"
      },
      "source": [
        "wandb.login()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aoTHl37XxAS9"
      },
      "source": [
        "## 1️⃣ Download the training data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_CEbECy3q0h"
      },
      "source": [
        "First, we need to download the training data\r\n",
        "onto the machine we're using.\r\n",
        "This same code will work on Google Cola and on your own machine.\r\n",
        "\r\n",
        "The data is stored as a Weights & Biases\r\n",
        "[Artifact](https://docs.wandb.ai/artifacts).\r\n",
        "The Artifacts system allows you\r\n",
        "to track the large binary files that are inputs to\r\n",
        "and outputs of machine learning experiments.\r\n",
        "Think of Artifacts like GitHub repositories,\r\n",
        "but for data and models instead of code!\r\n",
        "\r\n",
        "Your final submission in the contest\r\n",
        "will be in the form of an Artifact.\r\n",
        "Check out [this video tutorial](http://wandb.me/artifacts-video)\r\n",
        "to learn more about how to use Artifacts,\r\n",
        "or read the docs [here](https://docs.wandb.ai/artifacts/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cXF0H0kpVRAq"
      },
      "source": [
        "# picking out the training data artifact by name\r\n",
        "\r\n",
        "entity = \"charlesfrye\"  # artifacts are associated with an entity -- s user or team\r\n",
        "project = \"davis\"  # artifacts are associated with a project -- a collection of ML experiments\r\n",
        "split = \"train\"  # the train and val data are both stored in the same format\r\n",
        "tag = \"latest\"  # different versions of an Artifact have different tags\r\n",
        "\r\n",
        "training_data_artifact_id = os.path.join(entity, project, f\"davis2016-{split}\") + \":\" + tag\r\n",
        "training_data_artifact_id"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-XOfn-H6WX2"
      },
      "source": [
        "Calling `run.use_artifact` and then `.download()`\r\n",
        "during a script downloads the Artifact and its files to a local directory,\r\n",
        "if they aren't already present.\r\n",
        "\r\n",
        "This cell contains the minimal code you need to get the training data.\r\n",
        "Below, we'll see how to integrate Artifacts into your pipeline more fully,\r\n",
        "so that you can, e.g., track which inputs a model was trained on."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pyTXkGNEU8wo"
      },
      "source": [
        "with wandb.init(project=project, job_type=\"download\") as run:\r\n",
        "  training_data_artifact = run.use_artifact(training_data_artifact_id)\r\n",
        "  training_data_dir = training_data_artifact.download()\r\n",
        "  print(\"\\ntraining data downloaded to \" + training_data_dir)\r\n",
        "  !ls {training_data_dir}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aw9pYa8vxEGJ"
      },
      "source": [
        "### Dataset format and exploration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V-NUi6-DWYZ-"
      },
      "source": [
        "You can view the training data\r\n",
        "in the format used by all of the datasets,\r\n",
        "including the test set\r\n",
        "and submitted results,\r\n",
        "[here](http://wandb.me/davis-train-data).\r\n",
        "A short description of that format follows.\r\n",
        "\r\n",
        "Every artifact you use or make for the contest\r\n",
        "should have, at the top-level directory,\r\n",
        "a file called `paths.json`,\r\n",
        "which contains information on the paths to data files in the artifact.\r\n",
        "\r\n",
        "These files are intended to be read as\r\n",
        "[pandas `DataFrames`](https://pandas.pydata.org/).\r\n",
        "The resulting columns will possibly include\r\n",
        "- `\"raw\"`, for the input image files\r\n",
        "- `\"annotation\"`, for the ground truth segmentation masks, as PNG files, and\r\n",
        "- `\"output\"`, for model predictions. These will only be present for results saved as Artifacts.\r\n",
        "\r\n",
        "Note that the test set, when provided,\r\n",
        "will not have an `\"annotation\"` column,\r\n",
        "so make sure your model can run on datasets that don't have that column and only have `\"raw\"` images!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yI5OC8AB7L4G"
      },
      "source": [
        "![data-artifact-format](https://i.imgur.com/WQIXC0O.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVs2vcuVTMNr"
      },
      "source": [
        "The prefixes of paths are arbitrary and may have differing depths\r\n",
        "(the examples below have three directories,\r\n",
        "but other datasets may have a different number).\r\n",
        "\r\n",
        "However, every path will have, at the end, two elements:\r\n",
        "`{clip_name}/{12345}.jpg`\r\n",
        "where\r\n",
        "- `clip_name` is a string identifying the video clip to which the image belongs\r\n",
        "and\r\n",
        "- `12345` is a five-digit, [zero-filled](https://docs.python.org/3/library/stdtypes.html#str.zfill) number indicating the frame index of the image.\r\n",
        "\r\n",
        "The columns are assumed to be indexed by integers,\r\n",
        "and these integers are used\r\n",
        "to match `\"raw\"` and `\"annotation\"` in the starter code and\r\n",
        "to match `\"output\"` and `\"annotation\"`\r\n",
        "in the submission evaluation code.\r\n",
        "\r\n",
        "See below for an example."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oolt8W3S9C-8"
      },
      "source": [
        "![paths-content](https://i.imgur.com/Bh7EKte.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2JzspU6j7Khg"
      },
      "source": [
        "For convenience, the data has also been packaged up into a\r\n",
        "Weights & Biases Dataset Visualizaton Table [here](http://wandb.me/davis-train-table).\r\n",
        "This format, pictured below, is convenient for exploring the data\r\n",
        "and getting to know it better.\r\n",
        "\r\n",
        "You can read more about DSviz Tables [here](https://docs.wandb.ai/datasets-and-predictions)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9PTZ2RJG7LOY"
      },
      "source": [
        "![data-table-format](https://i.imgur.com/mliFzqc.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMuifb6FxJlB"
      },
      "source": [
        "## 2️⃣ Set up your data pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWjnjFJW_vcw"
      },
      "source": [
        "Now that the data is downloaded to the filesystem,\r\n",
        "we need to define a method for getting the data onto the GPU\r\n",
        "and into the model.\r\n",
        "\r\n",
        "This is much more complicated for big datasets,\r\n",
        "like this one, that can't fit inside the GPU\r\n",
        "comfortably alongside our model.\r\n",
        "\r\n",
        "In the [GitHub repo for this contest](https://github.com/wandb/davis-contest),\r\n",
        "we provide tools for loading data from disk using the\r\n",
        "[PyTorch Lightning library](https://pytorch-lightning.readthedocs.io/en/stable/),\r\n",
        "which helps organize and optimize complex PyTorch pipelines.\r\n",
        "\r\n",
        "For more on using PyTorch Lightning with Weights & Biases,\r\n",
        "check out\r\n",
        "[this tutorial video](http://wandb.me/lit-video)\r\n",
        "and [colab notebook](http://wandb.me/lit-colab)\r\n",
        "or read the [W&B docs](https://docs.wandb.ai/integrations/lightning)\r\n",
        "or the [PyTorch Lightning docs](https://pytorch-lightning.readthedocs.io/en/latest/api/pytorch_lightning.loggers.wandb.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JI-TJxN5A_U7"
      },
      "source": [
        "print(contest.torch.data.VidSegDataModule.__doc__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ePM-lvJBMNP"
      },
      "source": [
        "print(contest.torch.data.VidSegDataset.__doc__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_4_lxAg7B0iz"
      },
      "source": [
        "These tools load images without regard to which video they come from,\r\n",
        "and so it's difficult if not impossible to build a model that\r\n",
        "can make use of information over time,\r\n",
        "which is very useful for this task.\r\n",
        "\r\n",
        "One easy win over this baseline would be to rewrite this data-loading code\r\n",
        "to load clips and then construct a model architecture that makes use of temporal sequence information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3ZHadq5LAQ1"
      },
      "source": [
        "### Splitting up the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hj_W2MSzAqai"
      },
      "source": [
        "The small size of this dataset,\r\n",
        "relative to the difficulty of the task,\r\n",
        "increases the danger of over-fitting.\r\n",
        "\r\n",
        "To help track this during training,\r\n",
        "we'll split off some data into a holdout set\r\n",
        "and track our performance on that data.\r\n",
        "\r\n",
        "But we can't just randomly subsample specific frames,\r\n",
        "the way holdout sets are constructed in image datasets.\r\n",
        "That's because certain frames come from the same video,\r\n",
        "or _clip_, and holding out, say,\r\n",
        "every third frame from each clip\r\n",
        "doesn't prevent over-fitting nearly as effectively\r\n",
        "as holding out a third of the clips.\r\n",
        "\r\n",
        "To make working with clips easier,\r\n",
        "we provide utilities for splitting datasets\r\n",
        "at the level of clips.\r\n",
        "Use these tools as a blueprint\r\n",
        "for setting up your own tools that are \"clip-aware\" --\r\n",
        "for example, to build a model that makes use of temporal information."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-TLlyygKxZg"
      },
      "source": [
        "print(clips.split_on_clips.__doc__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGdM4UG8CgYH"
      },
      "source": [
        "The code below will create a random split into training and holdout validation data,\r\n",
        "at the level of clips, and then log the result\r\n",
        "to a Weights & Biases artifact.\r\n",
        "Notice the addition of a `paths.json` file to the artifact,\r\n",
        "so that it matches the format of other artifacts.\r\n",
        "\r\n",
        "`log_datasplit_artifact` demonstrates two steps needed to register an artifact on Weights & Biases:\r\n",
        "1. `add_file`s or `add_dir`s to the artifact to build it, and then\r\n",
        "2. upload the artifact to W&B servers using `run.log_artifact`.\r\n",
        "\r\n",
        "See the [documentation](https://docs.wandb.ai/artifacts/api)\r\n",
        "or the [tutorial video](http://wandb.me/artifacts-video) for more details on Artifacts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kkak4W9n_W1N"
      },
      "source": [
        "def log_holdout_split(data_artifact, train_split_df, holdout_split_df):\r\n",
        "  log_datasplit_artifact(data_artifact, train_split_df, \"train\")\r\n",
        "  log_datasplit_artifact(data_artifact, holdout_split_df, \"holdout\")\r\n",
        "\r\n",
        "\r\n",
        "def log_datasplit_artifact(data_artifact, split_df, splitname, folder=\"wandb\"):\r\n",
        "  dataset_artifact = wandb.Artifact(name=f\"davis2016-split-{splitname}\", type=\"split-data\")\r\n",
        "  path = os.path.join(folder, splitname + \".json\")\r\n",
        "  split_df.to_json(path)\r\n",
        "  # all artifacts in the contest need a paths.json file\r\n",
        "  dataset_artifact.add_file(path, \"paths.json\")\r\n",
        "\r\n",
        "  wandb.run.log_artifact(dataset_artifact)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SQqBg6VQENhV"
      },
      "source": [
        "config = {\"training_fraction\": 0.8}\r\n",
        "\r\n",
        "with wandb.init(project=project,\r\n",
        "                job_type=\"split-data\", config=config) as run:\r\n",
        "  training_data_artifact = run.use_artifact(training_data_artifact_id)\r\n",
        "  paths_df = paths.artifact_paths(training_data_artifact)\r\n",
        "\r\n",
        "  training_paths_df, holdout_paths_df = clips.split_on_clips(paths_df)\r\n",
        "  log_holdout_split(training_data_artifact,\r\n",
        "                    training_paths_df,\r\n",
        "                    holdout_paths_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93ja91LQEa0H"
      },
      "source": [
        "Notice also that this code makes use of the `training_data_artifact` with `run.use_artifact`.\r\n",
        "\r\n",
        "Logging where data came from\r\n",
        "(while simultaneously downloading it if need be!)\r\n",
        "makes it easier to understand and reproduce your work later,\r\n",
        "track down bugs or identify the cause of model regressions,\r\n",
        "and otherwise understand how the data influenced your model.\r\n",
        "\r\n",
        "For example, if you check the Artifacts tab\r\n",
        "on the run page for this run on Weights & Biases\r\n",
        "(see the auto-generated link produced when you run the cell below;\r\n",
        "the Artifacts tab is accessed by clicking the icon\r\n",
        "that looks like three hockey pucks in a stack),\r\n",
        "you can see which artifacts were used during the run\r\n",
        "and which were produced by it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dmwHmJpwE6i3"
      },
      "source": [
        "![artifact-io](https://i.imgur.com/Q7HzzF4.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "72hfXZf_F0Lb"
      },
      "source": [
        "This information is collated into a graph, as pictured below,\r\n",
        "that can be used to survey the entire pipeline of your project all at once.\r\n",
        "Use the Explode button to track individual runs and artifacts.\r\n",
        "\r\n",
        "This graph is\r\n",
        "accessible via the Graph View tab on an individual artifact's page\r\n",
        "(see [here](http://wandb.me/davis-artifacts-graph-eg) for an example).\r\n",
        "\r\n",
        "The Graph View is also covered in the [video tutorial for Artifacts](http://wandb.me/artifacts-video)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCkkCsMuFopY"
      },
      "source": [
        "![artifacts-dag](https://i.imgur.com/F5sQIjz.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jObVDerP_gCi"
      },
      "source": [
        "## 3️⃣ Define a model and train it"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Z04PFOlHhho"
      },
      "source": [
        "Now that the data pipeline is set up,\r\n",
        "we can define a model that consumes the data\r\n",
        "and learns the task.\r\n",
        "\r\n",
        "It will need to take in images of arbitrary shape\r\n",
        "and then return outputs of the same shape,\r\n",
        "with values between 0 and 1,\r\n",
        "with high values corresponding to\r\n",
        "pixels that are more likely to be a part of the segmentation mask.\r\n",
        "\r\n",
        "This notebook demonstrates the absolute simplest model\r\n",
        "that can be applied to this data:\r\n",
        "a spatial convolution that looks at only one pixel at a time.\r\n",
        "This is fed into a `sigmoid` nonlinearity\r\n",
        "so that the output values are normalized."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ab2u0qDYvUXE"
      },
      "source": [
        "### Model Code"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tI0BQGk9pEkc"
      },
      "source": [
        "import pytorch_lightning as pl\r\n",
        "import torch\r\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ih3OdWVCIxb8"
      },
      "source": [
        "Four of the methods defined for this model -- `__init__`, `forward`, `training_step`, and `configure_optimizers` --\r\n",
        "are the minimum required to define a PyTorch\r\n",
        "[LightningModule](https://pytorch-lightning.readthedocs.io/en/stable/lightning_module.html/),\r\n",
        "which augments a typical PyTorch module with\r\n",
        "some extra hooks and callbacks that enable\r\n",
        "flexible but organized training loops."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HEg0WkTTW3P0"
      },
      "source": [
        "class DummyModel(pl.LightningModule):\r\n",
        "\r\n",
        "  def __init__(self):\r\n",
        "    super().__init__()\r\n",
        "    self.conv = torch.nn.Conv2d(in_channels=3, out_channels=1, kernel_size=1)\r\n",
        "\r\n",
        "  def forward(self, xs):\r\n",
        "    return torch.sigmoid(self.conv(xs))\r\n",
        "\r\n",
        "  def training_step(self, batch, batch_idx):\r\n",
        "    loss = self.forward_on_batch(batch)\r\n",
        "    return loss\r\n",
        "\r\n",
        "  def validation_step(self, batch, batch_idx):\r\n",
        "    loss = self.forward_on_batch(batch)\r\n",
        "    return loss\r\n",
        "\r\n",
        "  def forward_on_batch(self, batch):\r\n",
        "    xs, ys = batch\r\n",
        "    y_hats = self.forward(xs)\r\n",
        "    loss = F.binary_cross_entropy(y_hats, ys)\r\n",
        "    return loss\r\n",
        "\r\n",
        "  def configure_optimizers(self):\r\n",
        "    return torch.optim.SGD(self.parameters(), lr=0.0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fF4sq2BrKaZk"
      },
      "source": [
        "### Training Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wO8KUImXJk15"
      },
      "source": [
        "The cell below uses a PyTorch Lightning\r\n",
        "[Trainer](https://pytorch-lightning.readthedocs.io/en/stable/trainer.html)\r\n",
        "to train the model.\r\n",
        "Trainers handle plumbing tasks like\r\n",
        "ensuring the data and the model are on the same device, e.g. the GPU,\r\n",
        "turning on and off DropOut and BatchNorm as needed,\r\n",
        "and coordinating the data module and the model.\r\n",
        "\r\n",
        "We also use the Weights & Biases integration with PyTorch Lightning,\r\n",
        "[`WandbLogger`](https://pytorch-lightning.readthedocs.io/en/stable/generated/pytorch_lightning.loggers.WandbLogger.html),\r\n",
        "to track training and log the run to W&B.\r\n",
        "Head to the run page\r\n",
        "(the link appears once you run the cell)\r\n",
        "to watch this information come in live\r\n",
        "or review it afterwards --\r\n",
        "system metrics, gradient information,\r\n",
        "and loss metrics all get logged without any extra effort.\r\n",
        "\r\n",
        "For more on using Weights & Biases with PyTorch Lightning,\r\n",
        "check out [this video tutorial](http://wandb.me/lit-video).\r\n",
        "\r\n",
        "Note that the model is saved as an artifact as well.\r\n",
        "This makes it easier to run the model on evaluation data,\r\n",
        "e.g. on a different machine,\r\n",
        "and will make it easier to share your model with the judges\r\n",
        "at the end of the contest."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hia3dlPkefU1"
      },
      "source": [
        "model_artifact_name = \"dummy-baseline\"\r\n",
        "\r\n",
        "config = {\"batch_size\": 32,\r\n",
        "          \"max_epochs\": 1,\r\n",
        "          \"gpus\": 1}\r\n",
        "\r\n",
        "with wandb.init(project=project, config=config, job_type=\"train\") as run:\r\n",
        "\r\n",
        "  training_data_artifact = run.use_artifact(training_data_artifact_id)\r\n",
        "  training_data_artifact.download()\r\n",
        "\r\n",
        "  trainsplit_artifact = run.use_artifact(\"davis2016-split-train:latest\")\r\n",
        "  trainsplit_paths = paths.get_paths(trainsplit_artifact)\r\n",
        "\r\n",
        "  holdoutsplit_artifact = run.use_artifact(\"davis2016-split-holdout:latest\")\r\n",
        "  holdoutsplit_paths = paths.get_paths(holdoutsplit_artifact)\r\n",
        "\r\n",
        "  datamodule = contest.torch.data.VidSegDataModule(\r\n",
        "      trainsplit_paths, holdoutsplit_paths,\r\n",
        "      batch_size=wandb.config[\"batch_size\"])\r\n",
        "  datamodule.setup()\r\n",
        "\r\n",
        "  model = DummyModel()\r\n",
        "  wandb.config[\"nparams\"] = contest.torch.profile.count_params(model)\r\n",
        "  wandb.config[\"nflops\"] = contest.torch.profile.count_flops(model, torch.cuda.device(0))\r\n",
        "  \r\n",
        "  logger = pl.loggers.wandb.WandbLogger(experiment=run)\r\n",
        "  logger.watch(model, log_freq=2)\r\n",
        "\r\n",
        "  trainer = pl.Trainer(\r\n",
        "    gpus=wandb.config[\"gpus\"], max_epochs=wandb.config[\"max_epochs\"],\r\n",
        "    logger=logger, log_every_n_steps=1)\r\n",
        "  \r\n",
        "  trainer.fit(model, datamodule)\r\n",
        "\r\n",
        "  model_artifact_id = contest.torch.utils.save_model_to_artifact(\r\n",
        "    model, \"wandb/final_model\", model_artifact_name)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7VeTiL7uKypw"
      },
      "source": [
        "Two things to point out here:\r\n",
        "1. Hyperparameters for the run are stored in the `config` dictionary at the top,\r\n",
        "which is passed to `wandb.init`. That way, the hyperparameter values are logged to W&B. These hyperparameters are then accessed using the `wandb.config` attribute. That way, you can be sure the logged values are the same as the values being used.\r\n",
        "2. Added to the `wandb.config` later, we have the `n`umber of `param`eters in the model, calculated using the `torch.profile` tools provided for the contest. While you don't need to track this during training, this information _must_ be included with your submission (as described in the next section) and be underneath the limits in the contest description,\r\n",
        "or else the submission is invalid.\r\n",
        "If your model's parameters cannot be counted with the methods we provide, you're responsible for ensuring they are counted correctly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y7gj0eF8yc2S"
      },
      "source": [
        "## 4️⃣ Run your model on the evaluation data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5eCpeEgWNFPN"
      },
      "source": [
        "As the contest runs,\r\n",
        "you can submit your performance on the validation data\r\n",
        "to be included on a public leaderboard.\r\n",
        "\r\n",
        "Final standings will be determined based on performance on a test set,\r\n",
        "not on this validation set.\r\n",
        "The test set will be released,\r\n",
        "without labels,\r\n",
        "in the last 72 hours of the contest\r\n",
        "(the \"testing phase\").\r\n",
        "During that time,\r\n",
        "participants will submit their model's results\r\n",
        "to be ranked on a private leaderboard.\r\n",
        "\r\n",
        "It's a well-known phenomenon that the best performers on validation data\r\n",
        "are not always the best performers on new test data,\r\n",
        "even in restricted settings like Kaggle competitions.\r\n",
        "The difficulty of the task and the heterogeneity of video data\r\n",
        "make this especially likely for this contest,\r\n",
        "as is common in production machine learning.\r\n",
        "\r\n",
        "In order to provide a framework-independent format for results\r\n",
        "that can be used for both validation and test data,\r\n",
        "the submission generation process has been split into two steps:\r\n",
        "1. Execute the model on the evaluation data, logging a \"result\" artifact and run to Weights & Biases with a specific structure, described below.\r\n",
        "2. Submit the results to a Weights & Biases benchmark\r\n",
        "\r\n",
        "During the training phase,\r\n",
        "use [this notebook](http://wandb.me/davis-submit)\r\n",
        "to submit to the benchmark.\r\n",
        "Once the testing phase opens, follow the provided instructions.\r\n",
        "\r\n",
        "Note that the validation data contains labels,\r\n",
        "but the test data will not!\r\n",
        "Take care to write your result generation code\r\n",
        "so that it will run even if no annotations are provided.\r\n",
        "\r\n",
        "We provide starter code for both PyTorch and Keras,\r\n",
        "using the simple dataloaders provided for the training loop above.\r\n",
        "If you change the data pipeline,\r\n",
        "you may need to write your own code here.\r\n",
        "You should aim to still produce a `pd.Series` of `output_paths`\r\n",
        "that can be passed to `contest.evaluate.make_result_artifact`\r\n",
        "so you can make sure your results are in the right format."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5xg9fMtO1OJ"
      },
      "source": [
        "evaluation_artifact_name = os.path.join(entity, project, \"davis2016-val\" +\":\" + tag)\n",
        "\n",
        "model_tag = \"latest\"\n",
        "\n",
        "result_artifact_name = model_artifact_name + \"-result\"\n",
        "\n",
        "output_dir = os.path.join(\"outputs\")\n",
        "!rm -rf output_dir\n",
        "!mkdir -p {output_dir}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZkNEH5zOOIJ-"
      },
      "source": [
        "with wandb.init(project=project, job_type=\"run-val\") as run:\r\n",
        "  evaluation_data_artifact = run.use_artifact(evaluation_artifact_name)\r\n",
        "  evaluation_data_paths = paths.artifact_paths(evaluation_data_artifact)\r\n",
        "\r\n",
        "  evaluation_dataset = contest.torch.data.VidSegDataset(\r\n",
        "    evaluation_data_paths, has_annotations=False)\r\n",
        "  num_images = len(evaluation_dataset)\r\n",
        "\r\n",
        "  evaluation_dataloader = torch.utils.data.DataLoader(\r\n",
        "    evaluation_dataset, batch_size=1)\r\n",
        "\r\n",
        "  model = contest.torch.utils.load_model_from_artifact(\r\n",
        "    model_artifact_name + \":\" + model_tag, DummyModel) \r\n",
        "\r\n",
        "  print(\"\\n\")\r\n",
        "  device = torch.cuda.device(0)\r\n",
        "  nparams = contest.torch.profile.count_params(model)\r\n",
        "  nflops = contest.torch.profile.count_flops(model, device)\r\n",
        "\r\n",
        "  # the number of parameters in the model must be logged\r\n",
        "  profiling_info = {\"nparams\": nparams, \"nflops\": nflops}\r\n",
        "  wandb.log(profiling_info)\r\n",
        "\r\n",
        "  output_paths = contest.torch.evaluate.run(\r\n",
        "    model, evaluation_dataloader, num_images, output_dir)\r\n",
        "\r\n",
        "  # the number of parameters in the model should also be included in the result artifact\r\n",
        "  result_artifact = contest.evaluate.make_result_artifact(\r\n",
        "    output_paths, result_artifact_name, metadata=profiling_info)\r\n",
        "  run.log_artifact(result_artifact)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pKYX3uWwPfWW"
      },
      "source": [
        "Check out the page for the result artifact associated with this run\r\n",
        "(link appears above after executing the cell)\r\n",
        "in order to see an example of a formatted result.\r\n",
        "\r\n",
        "A result artifact looks much like a dataset artifact\r\n",
        "-- it has a `paths.json` file,\r\n",
        "along with files that are pointed to by the contents of that file --\r\n",
        "but it need only contain a single key: `\"output\"`.\r\n",
        "The output files are black and white PNG files with unsigned 8-bit pixel values between 0 and 255 that represent the model's confidence that a given pixel\r\n",
        "in the image is part of the segmentation mask."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BpcIOZhMQuaB"
      },
      "source": [
        "Models are required to obey a parameter count constraint,\r\n",
        "and the parameter count information must be reported as part of the result.\r\n",
        "If your result does not have the parameter count both logged\r\n",
        "with the run and associated with the artifact,\r\n",
        "it will be declared invalid.\r\n",
        "See the discussion above in the model training section or\r\n",
        "[the GitHub repository for the contest](https://github.com/wandb/davis-contest)\r\n",
        "for more."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xQDEuHMzNjp"
      },
      "source": [
        "## 5️⃣ Submit your results to the leaderboard on Weights & Biases"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XGn2Y9wUU2W9"
      },
      "source": [
        "Once you've run an evaluation job like the one above and produced a results artifact,\r\n",
        "you're almost ready to submit to the contest.\r\n",
        "\r\n",
        "Head over to [this notebook](http://wandb.me/davis-submit) for the last two steps."
      ]
    }
  ]
}